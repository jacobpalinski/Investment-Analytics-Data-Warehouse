name: Scheduled CI/CD

on:
  push:
    branches:
      - dev
  #schedule:
    #- cron: "0 11 * * *"
    #- cron: "0 3 1 * *"
  #workflow_run:
    #workflows: ["Record Main Update"]
    #types:
      #- completed

jobs:
  #tickers_extraction:
    #if: github.event.schedule == "0 3 1 * *"
    #environment: prd
    #runs-on: ubuntu-latest

    #steps:
      #- name: Checkout repository
        #uses: actions/checkout@v4

      #- name: Set up Python
        #uses: actions/setup-python@v5
        #with:
          #python-version: "3.11"

      #- name: Install dependencies
        #run: pip install -r requirements.txt

      #- name: Run extract_listed_tickers.py
        #working-directory: ./airflow/dags/nasdaq_listed_tickers
        #run: python extract_listed_tickers.py

  check-and-deploy:
    #if: github.event.schedule == "0 11 * * *"
    environment: prd
    runs-on: ubuntu-latest

    defaults:
      run:
        working-directory: ./infrastructure

    env:
      AWS_ACCESS_KEY_ID: ${{secrets.AWS_ACCESS_KEY_ID}}
      AWS_SECRET: ${{secrets.AWS_SECRET}}
      AWS_REGION: ${{secrets.AWS_REGION}}
      AWS_S3_BUCKET: ${{secrets.AWS_S3_BUCKET}}
      AWS_S3_TST_BUCKET: ${{secrets.AWS_S3_TST_BUCKET}}
      POLYGON_API_KEY: ${{secrets.POLYGON_API_KEY}}
      FINNHUB_API_KEY: ${{secrets.FINNHUB_API_KEY}}
      NEWS_API_KEY: ${{secrets.NEWS_API_KEY}}
      REDDIT_CLIENT_ID: ${{secrets.REDDIT_CLIENT_ID}}
      REDDIT_CLIENT_SECRET: ${{secrets.REDDIT_CLIENT_SECRET}}
      REDDIT_USER_AGENT: ${{secrets.REDDIT_USER_AGENT}}
      REDDIT_USERNAME: ${{secrets.REDDIT_USERNAME}}
      REDDIT_PASSWORD: ${{secrets.REDDIT_PASSWORD}}
      SNOWFLAKE_USER: ${{secrets.SNOWFLAKE_USER}}
      SNOWFLAKE_PASSWORD: ${{secrets.SNOWFLAKE_PASSWORD}}
      SNOWFLAKE_ACCOUNT: ${{secrets.SNOWFLAKE_ACCOUNT}}
      SNOWFLAKE_ROLE: ${{secrets.SNOWFLAKE_ROLE}}
      FRED_API_KEY: ${{secrets.FRED_API_KEY}}
      SEC_API_USER_AGENT: ${{secrets.SEC_API_USER_AGENT}}
      SNOWFLAKE_PRIVATE_KEY_B64: ${{secrets.SNOWFLAKE_PRIVATE_KEY_B64}}
      SNOWFLAKE_PRIVATE_KEY_B64_FULL: ${{secrets.SNOWFLAKE_PRIVATE_KEY_B64_FULL}}
      SNOWFLAKE_PRIVATE_KEY_PASSPHRASE: ${{secrets.SNOWFLAKE_PRIVATE_KEY_PASSPHRASE}}
      KAFKA_BOOTSTRAP_SERVERS_TST: ${{secrets.KAFKA_BOOTSTRAP_SERVERS_TST}}
      KAFKA_TOPIC_TST: ${{secrets.KAFKA_TOPIC_TST}}
      SCHEMA_REGISTRY_URL_TST: ${{secrets.SCHEMA_REGISTRY_URL_TST}}
      METABASE_PRIVATE_KEY: ${{secrets.METABASE_PRIVATE_KEY}}
      METABASE_USERNAME: ${{secrets.METABASE_USERNAME}}
      METABASE_PASSWORD: ${{secrets.METABASE_PASSWORD}}
      TF_VAR_aws_access_key_id: ${{secrets.AWS_ACCESS_KEY_ID}}
      TF_VAR_aws_secret: ${{secrets.AWS_SECRET}}
      TF_VAR_aws_s3_bucket: ${{secrets.AWS_S3_BUCKET}}
      TF_VAR_aws_s3_tst_bucket: ${{secrets.AWS_S3_TST_BUCKET}}
      TF_VAR_airflow_uid: ${{secrets.AIRFLOW_UID}}
      TF_VAR_airflow_username: ${{secrets.AIRFLOW_USERNAME}}
      TF_VAR_airflow_password: ${{secrets.AIRFLOW_PASSWORD}}
      TF_VAR_polygon_api_key: ${{secrets.POLYGON_API_KEY}}
      TF_VAR_finnhub_api_key: ${{secrets.FINNHUB_API_KEY}}
      TF_VAR_news_api_key: ${{secrets.NEWS_API_KEY}}
      TF_VAR_reddit_client_id: ${{secrets.REDDIT_CLIENT_ID}}
      TF_VAR_reddit_client_secret: ${{secrets.REDDIT_CLIENT_SECRET}}
      TF_VAR_reddit_user_agent: ${{secrets.REDDIT_USER_AGENT}}
      TF_VAR_reddit_username: ${{secrets.REDDIT_USERNAME}}
      TF_VAR_reddit_password: ${{secrets.REDDIT_PASSWORD}}
      TF_VAR_snowflake_user: ${{secrets.SNOWFLAKE_USER}}
      TF_VAR_snowflake_password: ${{secrets.SNOWFLAKE_PASSWORD}}
      TF_VAR_snowflake_account: ${{secrets.SNOWFLAKE_ACCOUNT}}
      TF_VAR_snowflake_role: ${{secrets.SNOWFLAKE_ROLE}}
      TF_VAR_fred_api_key: ${{secrets.FRED_API_KEY}}
      TF_VAR_sec_api_user_agent: ${{secrets.SEC_API_USER_AGENT}}
      TF_VAR_snowflake_private_key_b64: ${{secrets.SNOWFLAKE_PRIVATE_KEY_B64}}
      TF_VAR_snowflake_private_key_b64_full: ${{secrets.SNOWFLAKE_PRIVATE_KEY_B64_FULL}}
      TF_VAR_snowflake_private_key_passphrase: ${{secrets.SNOWFLAKE_PRIVATE_KEY_PASSPHRASE}}
      TF_VAR_kafka_bootstrap_servers: ${{secrets.KAFKA_BOOTSTRAP_SERVERS}}
      TF_VAR_kafka_topic: ${{secrets.KAFKA_TOPIC}}
      TF_VAR_schema_registry_url: ${{secrets.SCHEMA_REGISTRY_URL}}
      TF_VAR_metabase_private_key: ${{secrets.METABASE_PRIVATE_KEY}}
      TF_VAR_metabase_username: ${{secrets.METABASE_USERNAME}}
      TF_VAR_metabase_password: ${{secrets.METABASE_PASSWORD}}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        working-directory: .
        run: pip install -r requirements.txt
      
      - name: Set up Terraform
        uses: hashicorp/setup-terraform@v3

      - name: Terraform Init
        run: terraform init

      - name: Terraform Plan
        run: terraform plan -out=tfplan

      - name: Terraform Apply
        run: terraform apply -auto-approve tfplan
      
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
      
      - name: Install Docker Compose
        run: sudo apt-get update && sudo apt-get install -y docker-compose
      
      - name: Start services with docker-compose
        working-directory: ./airflow/dags/tests
        run: docker-compose -f docker-compose-integration.yml up -d
      
      - name: Wait for services
        run: |
          echo "Waiting for containers..."
          sleep 90
          docker ps
      
      - name: Create kafka topic for testing
        working-directory: ./airflow/dags/tests
        run: docker exec tests_kafka-1_1 kafka-topics --bootstrap-server kafka-1:9092 --create --topic stock_aggregates_raw_tst --partitions 1 --replication-factor 1
      
      - name: Run tests
        working-directory: .
        run: pytest airflow/dags/tests -vv
      
      - name: Stop integration services
        working-directory: ./airflow/dags/tests
        run: docker-compose -f docker-compose-integration.yml down
      
      - name: Get EC2 instance ID
        run: |
          INSTANCE_ID=$(terraform output -raw ssm_instance_id)
          echo "INSTANCE_ID=$INSTANCE_ID" >> $GITHUB_ENV
      
      - name: Launch EC2 instance and create .env file with variables from SSM parameter store
        run: |
          echo "Deploying to EC2 instance: $INSTANCE_ID"
          aws ssm send-command \
            --region $AWS_REGION \
            --instance-ids "$INSTANCE_ID" \
            --document-name "AWS-RunShellScript" \
            --comment "Launch EC2 instance and create .env file" \
            --parameters '{ "commands": [
            "if [ -d Investment-Analytics-Data-Warehouse/.git ]; then cd Investment-Analytics-Data-Warehouse && git pull; else git clone https://github.com/jacobpalinski/Investment-Analytics-Data-Warehouse.git; cd Investment-Analytics-Data-Warehouse; fi",
            "echo \"\" > .env",
            "echo \"AWS_ACCESS_KEY_ID=$(aws ssm get-parameter --name \"/investment_analytics_data_warehouse/prd/ACCESS_KEY_ID\" --with-decryption --query \"Parameter.Value\" --output text)\" >> .env",
            "echo \"AWS_SECRET=$(aws ssm get-parameter --name \"/investment_analytics_data_warehouse/prd/SECRET\" --with-decryption --query \"Parameter.Value\" --output text)\" >> .env",
            "echo \"AWS_S3_BUCKET=$(aws ssm get-parameter --name \"/investment_analytics_data_warehouse/prd/S3_BUCKET\" --with-decryption --query \"Parameter.Value\" --output text)\" >> .env",
            "echo \"AWS_S3_TST_BUCKET=$(aws ssm get-parameter --name \"/investment_analytics_data_warehouse/prd/S3_TST_BUCKET\" --with-decryption --query \"Parameter.Value\" --output text)\" >> .env",
            "echo \"AIRFLOW_UID=$(aws ssm get-parameter --name \"/investment_analytics_data_warehouse/prd/AIRFLOW_UID\" --with-decryption --query \"Parameter.Value\" --output text)\" >> .env",
            "echo \"_AIRFLOW_WWW_USER_USERNAME=$(aws ssm get-parameter --name \"/investment_analytics_data_warehouse/prd/_AIRFLOW_WWW_USER_USERNAME\" --with-decryption --query \"Parameter.Value\" --output text)\" >> .env",
            "echo \"_AIRFLOW_WWW_USER_PASSWORD=$(aws ssm get-parameter --name \"/investment_analytics_data_warehouse/prd/_AIRFLOW_WWW_USER_PASSWORD\" --with-decryption --query \"Parameter.Value\" --output text)\" >> .env",
            "echo \"POLYGON_API_KEY=$(aws ssm get-parameter --name \"/investment_analytics_data_warehouse/prd/POLYGON_API_KEY\" --with-decryption --query \"Parameter.Value\" --output text)\" >> .env",
            "echo \"FINNHUB_API_KEY=$(aws ssm get-parameter --name \"/investment_analytics_data_warehouse/prd/FINNHUB_API_KEY\" --with-decryption --query \"Parameter.Value\" --output text)\" >> .env",
            "echo \"NEWS_API_KEY=$(aws ssm get-parameter --name \"/investment_analytics_data_warehouse/prd/NEWS_API_KEY\" --with-decryption --query \"Parameter.Value\" --output text)\" >> .env",
            "echo \"REDDIT_CLIENT_ID=$(aws ssm get-parameter --name \"/investment_analytics_data_warehouse/prd/REDDIT_CLIENT_ID\" --with-decryption --query \"Parameter.Value\" --output text)\" >> .env",
            "echo \"REDDIT_CLIENT_SECRET=$(aws ssm get-parameter --name \"/investment_analytics_data_warehouse/prd/REDDIT_CLIENT_SECRET\" --with-decryption --query \"Parameter.Value\" --output text)\" >> .env",
            "echo \"REDDIT_USER_AGENT=$(aws ssm get-parameter --name \"/investment_analytics_data_warehouse/prd/REDDIT_USER_AGENT\" --with-decryption --query \"Parameter.Value\" --output text)\" >> .env",
            "echo \"REDDIT_USERNAME=$(aws ssm get-parameter --name \"/investment_analytics_data_warehouse/prd/REDDIT_USERNAME\" --with-decryption --query \"Parameter.Value\" --output text)\" >> .env",
            "echo \"REDDIT_PASSWORD=$(aws ssm get-parameter --name \"/investment_analytics_data_warehouse/prd/REDDIT_PASSWORD\" --with-decryption --query \"Parameter.Value\" --output text)\" >> .env",
            "echo \"SNOWFLAKE_USER=$(aws ssm get-parameter --name \"/investment_analytics_data_warehouse/prd/SNOWFLAKE_USER\" --with-decryption --query \"Parameter.Value\" --output text)\" >> .env",
            "echo \"SNOWFLAKE_PASSWORD=$(aws ssm get-parameter --name \"/investment_analytics_data_warehouse/prd/SNOWFLAKE_PASSWORD\" --with-decryption --query \"Parameter.Value\" --output text)\" >> .env",
            "echo \"SNOWFLAKE_ACCOUNT=$(aws ssm get-parameter --name \"/investment_analytics_data_warehouse/prd/SNOWFLAKE_ACCOUNT\" --with-decryption --query \"Parameter.Value\" --output text)\" >> .env",
            "echo \"SNOWFLAKE_PRIVATE_KEY_B64=$(aws ssm get-parameter --name \"/investment_analytics_data_warehouse/prd/SNOWFLAKE_PRIVATE_KEY_B64\" --with-decryption --query \"Parameter.Value\" --output text)\" >> .env",
            "echo \"SNOWFLAKE_PRIVATE_KEY_B64_FULL=$(aws ssm get-parameter --name \"/investment_analytics_data_warehouse/prd/SNOWFLAKE_PRIVATE_KEY_B64_FULL\" --with-decryption --query \"Parameter.Value\" --output text)\" >> .env",
            "echo \"SNOWFLAKE_PRIVATE_KEY_PASSPHRASE=$(aws ssm get-parameter --name \"/investment_analytics_data_warehouse/prd/SNOWFLAKE_PRIVATE_KEY_PASSPHRASE\" --with-decryption --query \"Parameter.Value\" --output text)\" >> .env",
            "echo \"FRED_API_KEY=$(aws ssm get-parameter --name \"/investment_analytics_data_warehouse/prd/FRED_API_KEY\" --with-decryption --query \"Parameter.Value\" --output text)\" >> .env",
            "echo \"SEC_API_USER_AGENT=$(aws ssm get-parameter --name \"/investment_analytics_data_warehouse/prd/SEC_API_USER_AGENT\" --with-decryption --query \"Parameter.Value\" --output text)\" >> .env",
            "echo \"KAFKA_BOOTSTRAP_SERVERS=$(aws ssm get-parameter --name \"/investment_analytics_data_warehouse/prd/KAFKA_BOOTSTRAP_SERVERS\" --with-decryption --query \"Parameter.Value\" --output text)\" >> .env",
            "echo \"KAFKA_TOPIC=$(aws ssm get-parameter --name \"/investment_analytics_data_warehouse/prd/KAFKA_TOPIC\" --with-decryption --query \"Parameter.Value\" --output text)\" >> .env",
            "echo \"SCHEMA_REGISTRY_URL=$(aws ssm get-parameter --name \"/investment_analytics_data_warehouse/prd/SCHEMA_REGISTRY_URL\" --with-decryption --query \"Parameter.Value\" --output text)\" >> .env",
            "echo \"METABASE_PRIVATE_KEY=$(aws ssm get-parameter --name \"/investment_analytics_data_warehouse/prd/METABASE_PRIVATE_KEY\" --with-decryption --query \"Parameter.Value\" --output text)\" >> .env",
            "echo \"METABASE_USERNAME=$(aws ssm get-parameter --name \"/investment_analytics_data_warehouse/prd/METABASE_USERNAME\" --with-decryption --query \"Parameter.Value\" --output text)\" >> .env",
            "echo \"METABASE_PASSWORD=$(aws ssm get-parameter --name \"/investment_analytics_data_warehouse/prd/METABASE_PASSWORD\" --with-decryption --query \"Parameter.Value\" --output text)\" >> .env",            
            "chmod 600 .env"
            ]}'
      
      - name: Create virtual environment and install dependencies
        run: |
          echo "Creating virtual environment and installing dependencies"
          aws ssm send-command \
              --region $AWS_REGION \
              --instance-ids "$INSTANCE_ID" \
              --document-name "AWS-RunShellScript" \
              --comment "Create virtual environment and install dependencies" \
              --parameters '{ "commands": [
              "if [ ! -d venv ]; then python3 -m venv venv; fi",
              "source venv/bin/activate",
              "if [ -f requirements.txt ]; then pip install -r requirements.txt; fi"
              ] }'
      
      - name: Initialise Airflow and create AWS and Snowflake connections
        run: |
          echo "Initialise Airflow and create AWS and Snowflake connections"
          aws ssm send-command \
          --region "$AWS_REGION" \
          --instance-ids "$INSTANCE_ID" \
          --document-name "AWS-RunShellScript" \
          --comment "Initialise Airflow Docker containers and connections" \
          --parameters '{
            "commands": [
            "docker compose run --rm airflow-init",
            "docker compose up postgres redis airflow-apiserver airflow-scheduler airflow-dag-processor airflow-worker airflow-triggerer -d",
            "sleep 120",
            "docker exec investment-analytics-data-warehouse-airflow-scheduler-1 airflow connections add snowflake_default \
              --conn-type snowflake \
              --conn-login '"'"${SNOWFLAKE_USER}"'"' \
              --conn-password '"'"${SNOWFLAKE_PASSWORD}"'"' \
              --conn-account '"'"${SNOWFLAKE_ACCOUNT}"'"' \
              --conn-extra '\''{\"database\": \"'"INVESTMENT_ANALYTICS"'\", \"warehouse\": \"'"INVESTMENT_ANALYTICS_DWH"'\", \"role\": \"'"${SNOWFLAKE_ROLE}"'\"}'\''",
            "docker exec investment-analytics-data-warehouse-airflow-scheduler-1 airflow connections add aws_default \
              --conn-type aws \
              --conn-login '"'"${AWS_ACCESS_KEY_ID}"'"' \
              --conn-password '"'"${AWS_SECRET_ACCESS_KEY}"'"'"
            ]
          }'
      
      - name: Setup Kafka Containers
        run: |
          echo "Setup Kafka containers"
          aws ssm send-command \
          --region "$AWS_REGION" \
          --instance-ids "$INSTANCE_ID" \
          --document-name "AWS-RunShellScript" \
          --comment "Setup Kafka containers" \
          --parameters '{
          "commands": [
          "docker compose up -d zookeeper-1 zookeeper-2 zookeeper-3 kafka-1 kafka-2 kafka-3 schema-registry kafka-connect",
          "docker exec investment-analytics-data-warehouse-kafka-1 kafka-topics --bootstrap-server kafka-1:9092 --create --topic stock_aggregates_raw --partitions 1 --replication-factor 3",
          "cd streaming",
          "curl -X POST -H 'Content-Type: application/json' --data @connector.json http://localhost:8083/connectors"
          ]
          }'
      
      - name: Metabase login
        run: |
          echo "Login to Metabase"
          aws ssm send-command \
          --region "$AWS_REGION" \
          --instance-ids "$INSTANCE_ID" \
          --document-name "AWS-RunShellScript" \
          --comment "Metabase login" \
          --parameters '{
          "commands": [
          "curl -X POST -H 'Content-Type: application/json' -d '{"username": "$METABASE_USERNAME", "password": "$METABASE_PASSWORD"}'"
          }'



      







      