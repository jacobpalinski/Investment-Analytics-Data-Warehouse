name: Scheduled CI/CD

on:
  push:
    branches:
      - dev
  #schedule:
    #- cron: "0 11 * * *"
    #- cron: "0 3 1 * *"
  #workflow_run:
    #workflows: ["Record Main Update"]
    #types:
      #- completed

jobs:
  #tickers_extraction:
    #if: github.event.schedule == "0 3 1 * *"
    #environment: prd
    #runs-on: ubuntu-latest

    #steps:
      #- name: Checkout repository
        #uses: actions/checkout@v4

      #- name: Set up Python
        #uses: actions/setup-python@v5
        #with:
          #python-version: "3.11"

      #- name: Install dependencies
        #run: pip install -r requirements.txt

      #- name: Run extract_listed_tickers.py
        #working-directory: ./airflow/dags/nasdaq_listed_tickers
        #run: python extract_listed_tickers.py

  check-and-deploy:
    #if: github.event.schedule == "0 11 * * *"
    environment: prd
    runs-on: ubuntu-latest

    defaults:
      run:
        working-directory: ./infrastructure

    env:
      AWS_ACCESS_KEY_ID: ${{secrets.AWS_ACCESS_KEY_ID}}
      AWS_SECRET_ACCESS_KEY: ${{secrets.AWS_SECRET_ACCESS_KEY}}
      AWS_REGION: ${{secrets.AWS_REGION}}
      AWS_S3_BUCKET: ${{secrets.AWS_S3_BUCKET}}
      AWS_S3_TST_BUCKET: ${{secrets.AWS_S3_TST_BUCKET}}
      POLYGON_API_KEY: ${{secrets.POLYGON_API_KEY}}
      FINNHUB_API_KEY: ${{secrets.FINNHUB_API_KEY}}
      NEWS_API_KEY: ${{secrets.NEWS_API_KEY}}
      REDDIT_CLIENT_ID: ${{secrets.REDDIT_CLIENT_ID}}
      REDDIT_CLIENT_SECRET: ${{secrets.REDDIT_CLIENT_SECRET}}
      REDDIT_USER_AGENT: ${{secrets.REDDIT_USER_AGENT}}
      REDDIT_USERNAME: ${{secrets.REDDIT_USERNAME}}
      REDDIT_PASSWORD: ${{secrets.REDDIT_PASSWORD}}
      SNOWFLAKE_USER: ${{secrets.SNOWFLAKE_USER}}
      SNOWFLAKE_PASSWORD: ${{secrets.SNOWFLAKE_PASSWORD}}
      SNOWFLAKE_ACCOUNT: ${{secrets.SNOWFLAKE_ACCOUNT}}
      SNOWFLAKE_ROLE: ${{secrets.SNOWFLAKE_ROLE}}
      FRED_API_KEY: ${{secrets.FRED_API_KEY}}
      SEC_API_USER_AGENT: ${{secrets.SEC_API_USER_AGENT}}
      SNOWFLAKE_PRIVATE_KEY_B64: ${{secrets.SNOWFLAKE_PRIVATE_KEY_B64}}
      SNOWFLAKE_PRIVATE_KEY_B64_FULL: ${{secrets.SNOWFLAKE_PRIVATE_KEY_B64_FULL}}
      SNOWFLAKE_PRIVATE_KEY_PASSPHRASE: ${{secrets.SNOWFLAKE_PRIVATE_KEY_PASSPHRASE}}
      KAFKA_BOOTSTRAP_SERVERS_TST: ${{secrets.KAFKA_BOOTSTRAP_SERVERS_TST}}
      KAFKA_TOPIC_TST: ${{secrets.KAFKA_TOPIC_TST}}
      SCHEMA_REGISTRY_URL_TST: ${{secrets.SCHEMA_REGISTRY_URL_TST}}
      METABASE_PRIVATE_KEY: ${{secrets.METABASE_PRIVATE_KEY}}
      METABASE_USERNAME: ${{secrets.METABASE_USERNAME}}
      METABASE_PASSWORD: ${{secrets.METABASE_PASSWORD}}
      TF_VAR_aws_access_key_id: ${{secrets.AWS_ACCESS_KEY_ID}}
      TF_VAR_aws_secret_access_key: ${{secrets.AWS_SECRET_ACCESS_KEY}}
      TF_VAR_aws_s3_bucket: ${{secrets.AWS_S3_BUCKET}}
      TF_VAR_aws_s3_tst_bucket: ${{secrets.AWS_S3_TST_BUCKET}}
      TF_VAR_airflow_uid: ${{secrets.AIRFLOW_UID}}
      TF_VAR_airflow_username: ${{secrets.AIRFLOW_USERNAME}}
      TF_VAR_airflow_password: ${{secrets.AIRFLOW_PASSWORD}}
      TF_VAR_polygon_api_key: ${{secrets.POLYGON_API_KEY}}
      TF_VAR_finnhub_api_key: ${{secrets.FINNHUB_API_KEY}}
      TF_VAR_news_api_key: ${{secrets.NEWS_API_KEY}}
      TF_VAR_reddit_client_id: ${{secrets.REDDIT_CLIENT_ID}}
      TF_VAR_reddit_client_secret: ${{secrets.REDDIT_CLIENT_SECRET}}
      TF_VAR_reddit_user_agent: ${{secrets.REDDIT_USER_AGENT}}
      TF_VAR_reddit_username: ${{secrets.REDDIT_USERNAME}}
      TF_VAR_reddit_password: ${{secrets.REDDIT_PASSWORD}}
      TF_VAR_snowflake_user: ${{secrets.SNOWFLAKE_USER}}
      TF_VAR_snowflake_password: ${{secrets.SNOWFLAKE_PASSWORD}}
      TF_VAR_snowflake_account: ${{secrets.SNOWFLAKE_ACCOUNT}}
      TF_VAR_snowflake_role: ${{secrets.SNOWFLAKE_ROLE}}
      TF_VAR_fred_api_key: ${{secrets.FRED_API_KEY}}
      TF_VAR_sec_api_user_agent: ${{secrets.SEC_API_USER_AGENT}}
      TF_VAR_snowflake_private_key_b64: ${{secrets.SNOWFLAKE_PRIVATE_KEY_B64}}
      TF_VAR_snowflake_private_key_b64_full: ${{secrets.SNOWFLAKE_PRIVATE_KEY_B64_FULL}}
      TF_VAR_snowflake_private_key_passphrase: ${{secrets.SNOWFLAKE_PRIVATE_KEY_PASSPHRASE}}
      TF_VAR_kafka_bootstrap_servers: ${{secrets.KAFKA_BOOTSTRAP_SERVERS}}
      TF_VAR_kafka_topic: ${{secrets.KAFKA_TOPIC}}
      TF_VAR_schema_registry_url: ${{secrets.SCHEMA_REGISTRY_URL}}
      TF_VAR_metabase_private_key: ${{secrets.METABASE_PRIVATE_KEY}}
      TF_VAR_metabase_username: ${{secrets.METABASE_USERNAME}}
      TF_VAR_metabase_password: ${{secrets.METABASE_PASSWORD}}

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install dependencies
        working-directory: .
        run: pip install -r requirements.txt
      
      - name: Set up Terraform
        uses: hashicorp/setup-terraform@v3

      - name: Terraform Init
        run: terraform init

      - name: Terraform Plan
        run: terraform plan -out=tfplan

      - name: Terraform Apply
        run: terraform apply -auto-approve tfplan
      
      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3
      
      - name: Install Docker Compose
        run: sudo apt-get update && sudo apt-get install -y docker-compose
      
      - name: Start services with docker-compose
        working-directory: ./airflow/dags/tests
        run: docker-compose -f docker-compose-integration.yml up -d
      
      - name: Wait for services
        run: |
          echo "Waiting for containers..."
          sleep 90
          docker ps
      
      - name: Create kafka topic for testing
        working-directory: ./airflow/dags/tests
        run: docker exec tests_kafka-1_1 kafka-topics --bootstrap-server kafka-1:9092 --create --topic stock_aggregates_tst --partitions 1 --replication-factor 1
      
      - name: Create kafka Snowflake connection for testing
        working-directory: ./airflow/dags/tests
        run: |
          curl -X POST -H 'Content-Type: application/json' --data @connector.json http://localhost:8083/connectors
      
      - name: Run tests
        working-directory: .
        run: pytest airflow/dags/tests -vv
      
      - name: Stop integration services
        working-directory: ./airflow/dags/tests
        run: docker-compose -f docker-compose-integration.yml down
      
      - name: Get EC2 instance ID
        run: |
          INSTANCE_ID=$(terraform output -raw ssm_instance_id)
          echo "INSTANCE_ID=$INSTANCE_ID" >> $GITHUB_ENV
      
      - name: Create virtual environment and install dependencies
        run: |
          echo "Creating virtual environment and installing dependencies"
          aws ssm send-command \
              --region $AWS_REGION \
              --instance-ids "$INSTANCE_ID" \
              --document-name "AWS-RunShellScript" \
              --comment "Create virtual environment and install dependencies" \
              --parameters '{ "commands": [
              "sudo apt update",
              "sudo apt install ca-certificates curl",
              "sudo install -m 0755 -d /etc/apt/keyrings",
              "sudo curl -fsSL https://download.docker.com/linux/ubuntu/gpg -o /etc/apt/keyrings/docker.asc",
              "sudo chmod a+r /etc/apt/keyrings/docker.asc",
              "sudo tee /etc/apt/sources.list.d/docker.sources <<EOF\nTypes: deb\nURIs: https://download.docker.com/linux/ubuntu\nSuites: $(. /etc/os-release && echo \"${UBUNTU_CODENAME:-$VERSION_CODENAME}\")\nComponents: stable\nSigned-By: /etc/apt/keyrings/docker.asc\nEOF",
              "sudo apt update",
              "sudo apt install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin",
              "sudo systemctl enable docker",
              "sudo systemctl start docker",
              "sudo apt install -y python3-venv python3-pip",
              "if [ ! -d venv ]; then python3 -m venv venv; fi",
              "source venv/bin/activate",
              "if [ -f requirements.txt ]; then pip install -r requirements.txt; fi"
              ] }'
      
      - name: Launch EC2 instance and create .env file with variables from SSM parameter store
        run: |
          echo "Deploying to EC2 instance: $INSTANCE_ID"
          aws ssm send-command \
            --region $AWS_REGION \
            --instance-ids "$INSTANCE_ID" \
            --document-name "AWS-RunShellScript" \
            --comment "Launch EC2 instance and create .env file" \
            --parameters '{ "commands": [
            "if [ -d Investment-Analytics-Data-Warehouse/.git ]; then cd Investment-Analytics-Data-Warehouse && git fetch origin && git checkout dev && git pull origin dev; else git clone --branch dev https://github.com/jacobpalinski/Investment-Analytics-Data-Warehouse.git; cd Investment-Analytics-Data-Warehouse; fi",
            "echo \"\" > .env",
            "AWS_ACCESS_KEY_ID=$(aws ssm get-parameter --name /investment_analytics_data_warehouse/prd/ACCESS_KEY_ID --with-decryption --query Parameter.Value --output text)",
            "AWS_SECRET=$(aws ssm get-parameter --name /investment_analytics_data_warehouse/prd/SECRET --with-decryption --query Parameter.Value --output text)",
            "AWS_S3_BUCKET=$(aws ssm get-parameter --name /investment_analytics_data_warehouse/prd/S3_BUCKET --with-decryption --query Parameter.Value --output text)",
            "AWS_S3_TST_BUCKET=$(aws ssm get-parameter --name /investment_analytics_data_warehouse/prd/S3_TST_BUCKET --with-decryption --query Parameter.Value --output text)",
            "AIRFLOW_UID=$(aws ssm get-parameter --name /investment_analytics_data_warehouse/prd/AIRFLOW_UID --with-decryption --query Parameter.Value --output text)",
            "_AIRFLOW_WWW_USER_USERNAME=$(aws ssm get-parameter --name /investment_analytics_data_warehouse/prd/_AIRFLOW_WWW_USER_USERNAME --with-decryption --query Parameter.Value --output text)",
            "_AIRFLOW_WWW_USER_PASSWORD=$(aws ssm get-parameter --name /investment_analytics_data_warehouse/prd/_AIRFLOW_WWW_USER_PASSWORD --with-decryption --query Parameter.Value --output text)",
            "POLYGON_API_KEY=$(aws ssm get-parameter --name /investment_analytics_data_warehouse/prd/POLYGON_API_KEY --with-decryption --query Parameter.Value --output text)",
            "FINNHUB_API_KEY=$(aws ssm get-parameter --name /investment_analytics_data_warehouse/prd/FINNHUB_API_KEY --with-decryption --query Parameter.Value --output text)",
            "NEWS_API_KEY=$(aws ssm get-parameter --name /investment_analytics_data_warehouse/prd/NEWS_API_KEY --with-decryption --query Parameter.Value --output text)",
            "REDDIT_CLIENT_ID=$(aws ssm get-parameter --name /investment_analytics_data_warehouse/prd/REDDIT_CLIENT_ID --with-decryption --query Parameter.Value --output text)",
            "REDDIT_CLIENT_SECRET=$(aws ssm get-parameter --name /investment_analytics_data_warehouse/prd/REDDIT_CLIENT_SECRET --with-decryption --query Parameter.Value --output text)",
            "REDDIT_USER_AGENT=$(aws ssm get-parameter --name /investment_analytics_data_warehouse/prd/REDDIT_USER_AGENT --with-decryption --query Parameter.Value --output text)",
            "REDDIT_USERNAME=$(aws ssm get-parameter --name /investment_analytics_data_warehouse/prd/REDDIT_USERNAME --with-decryption --query Parameter.Value --output text)",
            "REDDIT_PASSWORD=$(aws ssm get-parameter --name /investment_analytics_data_warehouse/prd/REDDIT_PASSWORD --with-decryption --query Parameter.Value --output text)",
            "SNOWFLAKE_USER=$(aws ssm get-parameter --name /investment_analytics_data_warehouse/prd/SNOWFLAKE_USER --with-decryption --query Parameter.Value --output text)",
            "SNOWFLAKE_PASSWORD=$(aws ssm get-parameter --name /investment_analytics_data_warehouse/prd/SNOWFLAKE_PASSWORD --with-decryption --query Parameter.Value --output text)",
            "SNOWFLAKE_ACCOUNT=$(aws ssm get-parameter --name /investment_analytics_data_warehouse/prd/SNOWFLAKE_ACCOUNT --with-decryption --query Parameter.Value --output text)",
            "SNOWFLAKE_PRIVATE_KEY_B64=$(aws ssm get-parameter --name /investment_analytics_data_warehouse/prd/SNOWFLAKE_PRIVATE_KEY_B64 --with-decryption --query Parameter.Value --output text)",
            "SNOWFLAKE_PRIVATE_KEY_B64_FULL=$(aws ssm get-parameter --name /investment_analytics_data_warehouse/prd/SNOWFLAKE_PRIVATE_KEY_B64_FULL --with-decryption --query Parameter.Value --output text)",
            "SNOWFLAKE_PRIVATE_KEY_PASSPHRASE=$(aws ssm get-parameter --name /investment_analytics_data_warehouse/prd/SNOWFLAKE_PRIVATE_KEY_PASSPHRASE --with-decryption --query Parameter.Value --output text)",
            "FRED_API_KEY=$(aws ssm get-parameter --name /investment_analytics_data_warehouse/prd/FRED_API_KEY --with-decryption --query Parameter.Value --output text)",
            "SEC_API_USER_AGENT=$(aws ssm get-parameter --name /investment_analytics_data_warehouse/prd/SEC_API_USER_AGENT --with-decryption --query Parameter.Value --output text)",
            "KAFKA_BOOTSTRAP_SERVERS=$(aws ssm get-parameter --name /investment_analytics_data_warehouse/prd/KAFKA_BOOTSTRAP_SERVERS --with-decryption --query Parameter.Value --output text)",
            "KAFKA_TOPIC=$(aws ssm get-parameter --name /investment_analytics_data_warehouse/prd/KAFKA_TOPIC --with-decryption --query Parameter.Value --output text)",
            "SCHEMA_REGISTRY_URL=$(aws ssm get-parameter --name /investment_analytics_data_warehouse/prd/SCHEMA_REGISTRY_URL --with-decryption --query Parameter.Value --output text)",
            "METABASE_PRIVATE_KEY=$(aws ssm get-parameter --name /investment_analytics_data_warehouse/prd/METABASE_PRIVATE_KEY --with-decryption --query Parameter.Value --output text)",
            "METABASE_USERNAME=$(aws ssm get-parameter --name /investment_analytics_data_warehouse/prd/METABASE_USERNAME --with-decryption --query Parameter.Value --output text)",
            "METABASE_PASSWORD=$(aws ssm get-parameter --name /investment_analytics_data_warehouse/prd/METABASE_PASSWORD --with-decryption --query Parameter.Value --output text)",            
            "chmod 600 .env"
            ]}'
      
      - name: Initialise Airflow and create AWS and Snowflake connections
        run: |
          echo "Initialise Airflow and create AWS and Snowflake connections"
          aws ssm send-command \
            --region "$AWS_REGION" \
            --instance-ids "$INSTANCE_ID" \
            --document-name "AWS-RunShellScript" \
            --comment "Initialise Airflow Docker containers and connections" \
            --parameters "$(cat <<'EOF'
            {
            "commands": [
            "docker compose run --rm airflow-init",
            "docker compose up postgres redis airflow-apiserver airflow-scheduler airflow-dag-processor airflow-worker airflow-triggerer -d",
            "sleep 120",
            "docker exec investment-analytics-data-warehouse-airflow-scheduler-1 airflow connections add snowflake_default --conn-type snowflake --conn-login \"$SNOWFLAKE_USER\" --conn-password \"$SNOWFLAKE_PASSWORD\" --conn-account \"$SNOWFLAKE_ACCOUNT\" --conn-extra '{\"database\":\"INVESTMENT_ANALYTICS\",\"warehouse\":\"INVESTMENT_ANALYTICS_DWH\",\"role\":\"$SNOWFLAKE_ROLE\"}'",
            "docker exec investment-analytics-data-warehouse-airflow-scheduler-1 airflow connections add aws_default --conn-type aws --conn-login \"$AWS_ACCESS_KEY_ID\" --conn-password \"$AWS_SECRET_ACCESS_KEY\""
          ]
          }
          EOF
          )"
      
      - name: Setup Kafka Containers
        run: |
          echo "Setup Kafka containers"
          aws ssm send-command \
          --region "$AWS_REGION" \
          --instance-ids "$INSTANCE_ID" \
          --document-name "AWS-RunShellScript" \
          --comment "Setup Kafka containers" \
          --parameters '{
          "commands": [
          "docker compose up -d zookeeper-1 zookeeper-2 zookeeper-3 kafka-1 kafka-2 kafka-3 schema-registry kafka-connect",
          "docker exec investment-analytics-data-warehouse-kafka-1 kafka-topics --bootstrap-server kafka-1:9092 --create --topic stock_aggregates_raw --partitions 1 --replication-factor 3",
          "cd streaming && curl -X POST -H \"Content-Type: application/json\" --data @connector.json http://localhost:8083/connectors"
          ]
          }'
      
      - name: Metabase login
        run: |
          echo "Login to Metabase"
          aws ssm send-command \
          --region "$AWS_REGION" \
          --instance-ids "$INSTANCE_ID" \
          --document-name "AWS-RunShellScript" \
          --comment "Metabase login" \
          --parameters "{
          \"commands\": [
          \"curl -X POST -H \\\"Content-Type: application/json\\\" \
          -d '{\\\"username\\\": \\\"$METABASE_USERNAME\\\", \\\"password\\\": \\\"$METABASE_PASSWORD\\\"}' \
          http://localhost:3000/api/session\"
          ]
          }"



      







      